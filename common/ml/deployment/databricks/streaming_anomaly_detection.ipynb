{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"987f14fb-ddf1-406c-a267-e12aba10979a","showTitle":false,"title":""}},"source":["### ML scoring on Streaming Data \n","\n","#### Installation: This cluster needs to be installed with following libraries:\n","- Maven: org.apache.kafka:kafka-clients:3.3.1\n","- Pypi:\n","  - azure-ai-ml\n","  - mlflow\n","  - scikit-learn==1.0.2\n","  - azure-kusto-ingest==4.0.0\n","  - azure-kusto-data==4.0.0\n","  - torch==1.12.0"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7fcf0722-9a82-42f3-8b5a-2f2989f69989","showTitle":false,"title":""}},"source":["### Connect to Event Hubs"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"deb29e48-84e1-4fb5-a1fd-ab1a7e53e463","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import from_json, col,explode, split,get_json_object\n","from pyspark.sql.types import *\n","con_str = dbutils.secrets.get(\"scope01\", \"ehns001-con\")\n","EH_SASL = f\"org.apache.kafka.common.security.plain.PlainLoginModule required username='$ConnectionString' password='{con_str}';\"\n","GROUP_ID = \"$Default\"\n","\n","\n","data_schema = StructType([\n","    StructField(\"id\", StringType(), True),\n","    StructField(\"starttime\", TimestampType(), True),\n","    StructField(\"endtime\", TimestampType(), True),\n","    StructField(\"car_type\", StringType(), True),\n","    StructField(\"location\", StringType(), True),\n","   StructField(\"car_id\", StringType(), True)\n","]\n",")\n","availability = spark \\\n","  .readStream \\\n","  .format(\"kafka\") \\\n","  .option(\"kafka.bootstrap.servers\", \"ehns001.servicebus.windows.net:9093\") \\\n","  .option(\"subscribe\", \"availability\") \\\n","  .option(\"kafka.sasl.mechanism\",\"PLAIN\") \\\n","  .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n","  .option(\"kafka.sasl.jaas.config\", EH_SASL ) \\\n","  .option(\"kafka.request.timeout.ms\", \"60000\") \\\n","  .option(\"kafka.session.timeout.ms\", \"60000\") \\\n","  .option(\"kafka.group.id\", GROUP_ID) \\\n","  .option(\"failOnDataLoss\", \"false\") \\\n","  .option(\"minOffsetsPerTrigger\", 200) \\\n","  .load() \\\n","  .select(from_json(col(\"value\").cast(\"string\"), data_schema).alias(\"value\"), \"partition\") \\\n","  .select(\"value.id\",\"value.starttime\",\"value.endtime\",\"value.car_type\",\"value.location\",\"value.car_id\",\"partition\") \\\n","\n","availability.createOrReplaceTempView(\"availability\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"9eda8bf6-acb3-461f-93de-05a2e43f44c2","showTitle":false,"title":""}},"outputs":[],"source":["%sql select to_json(struct(location,starttime,car_type, partition)) value, car_id key from availability"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3df53dc1-0d49-4fd7-a5e4-061f51ac39c9","showTitle":false,"title":""}},"source":["### Login to Azure ML workspace to download model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ab133c9d-6046-4800-919a-e3f5d14f9e0a","showTitle":false,"title":""}},"outputs":[],"source":["#import required libraries\n","from azure.ai.ml import MLClient\n","from azure.identity import DefaultAzureCredential,DeviceCodeCredential\n","\n","#Enter details of your AzureML workspace\n","subscription_id = '840b5c5c-3f4a-459a-94fc-6bad2a969f9d'\n","resource_group = 'ml'\n","workspace = 'ws02ent'\n","\n","#connect to the workspace\n","ml_client = MLClient(DeviceCodeCredential(tenant_id=\"0fbe7234-45ea-498b-b7e4-1a8b2d3be4d9\"), subscription_id, resource_group, workspace)\n","ml_client.models.download(\"torch_autoencoder\", version=3,download_path=\"/dbfs/models\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"be81a767-edfd-4e8a-9fdb-3daafb7deb60","showTitle":false,"title":""}},"source":["### Autoencoder Scoring and write output to ADX and anomaly to EventHub"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c46dd302-01b1-4a1e-8eb3-1fde41568707","showTitle":false,"title":""}},"source":["##### The below uses your own account but you can also use a service principal instead"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ccd54765-957c-4615-9d63-1a5c80740c38","showTitle":false,"title":""}},"outputs":[],"source":["import mlflow\n","import torch as T\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import make_column_transformer\n","from azure.kusto.data import KustoClient, KustoConnectionStringBuilder\n","from azure.kusto.ingest import (\n","    QueuedIngestClient,\n","    IngestionProperties,\n","    KustoStreamingIngestClient,\n","    ManagedStreamingIngestClient\n","\n",")\n","THRESHOLD = 20\n","TIME_STEPS = 20\n","\n","\n","database_name =\"db01\"\n","table_name = \"anomaly_output\"\n","client_id = dbutils.secrets.get(\"scope01\", \"app01-id\")\n","client_secret = dbutils.secrets.get(\"scope01\", \"app01-sec\")\n","authority_id = \"0fbe7234-45ea-498b-b7e4-1a8b2d3be4d9\"\n","\n","schema = \"anomaly integer, location string, car_type string, count integer,timestamp timestamp\"\n","\n","class Autoencoder(T.nn.Module):  # 65-32-8-32-65\n","  def __init__(self):\n","    super(Autoencoder, self).__init__()\n","    self.fc1 = T.nn.Conv2d(1,32,7)\n","    self.fc2 = T.nn.Conv2d(32,16,7)\n","    self.fc3 = T.nn.ConvTranspose2d(16,32,7)\n","    self.fc4 = T.nn.ConvTranspose2d(32,1,7)\n","    # self.fc5 = T.nn.ConvTranspose2d(32,1,7)\n","\n","  def encode(self, x):  # 65-32-8\n","    z = T.tanh(self.fc1(x))\n","    z = T.tanh(self.fc2(z))  # latent in [-1,+1]\n","    return z  \n","\n","  def decode(self, x):  # 8-32-65\n","    z = T.tanh(self.fc3(x))\n","    z = T.sigmoid(self.fc4(z))\n","    # z = T.sigmoid(self.fc5(z))  # [0.0, 1.0]\n","    return z\n","    \n","  def forward(self, x):\n","    z = self.encode(x) \n","    z = self.decode(z) \n","    return z  # in [0.0, 1.0]\n","# Called when the service is loaded\n","def create_sequences(values, time_steps=TIME_STEPS):      \n","    output = []  \n","    for i in range(len(values) - time_steps + 1):\n","        output.append(values[i : (i + time_steps)])\n","    return np.stack(output)\n","def pre_process(df, transformer):\n","       input_df = df[['location','car_type', 'count']]\n","       mean = 2.2031823072902032e-17\n","       std = 1.0\n","       input_df['count'] = (input_df['count']- mean)/std\n","       transformed_data= transformer.transform(input_df)\n","       try:\n","         transformed_input = create_sequences(transformed_data)\n","       except Exception as e:\n","         raise Exception(\"transformed_input shape\", df.shape)\n","       transformed_input = np.expand_dims(transformed_input, 1)\n","       transformed_input = T.tensor(np.float32(transformed_input), dtype=T.float32).to(\"cpu\")\n","       return transformed_input\n","\n","\n","\n","def detect_anomaly_autoencoder(df):\n","  model = Autoencoder()\n","  model.load_state_dict(T.load(\"/dbfs/models/torch_autoencoder/autoencoder.json\"))\n","  model.eval()\n","  #load transformer\n","  locations =['loc_0', 'loc_1', 'loc_10', 'loc_11', 'loc_12', 'loc_13', 'loc_14',\n","      'loc_15', 'loc_16', 'loc_17', 'loc_18', 'loc_19', 'loc_2', 'loc_3',\n","      'loc_4', 'loc_5', 'loc_6', 'loc_7', 'loc_8', 'loc_9']\n","  car_types =['comfort', 'green', 'x', 'xl','comfort', 'green', 'x', 'xl','comfort', 'green', 'x', 'xl','comfort', 'green', 'x', 'xl','comfort', 'green', 'x', 'xl']\n","\n","  transformer = make_column_transformer(\n","      (OneHotEncoder(sparse=False), ['location', 'car_type']),\n","      remainder='passthrough')\n","  transformer.fit(pd.DataFrame({\"location\":locations, \"car_type\":car_types, \"count\":range(20)}))\n","\n","  if df.shape[0] < TIME_STEPS:\n","    df[\"anomaly\"] = 99\n","    return df\n","\n","  transformed_data = pre_process(df,transformer)\n","  Y = model(transformed_data)  # should be same as X\n","  errs = T.sum((transformed_data-Y)*(transformed_data-Y), dim=[1,2,3]).detach().numpy().tolist()  #\n","  anomalies = [int(err>THRESHOLD) for err in errs]\n","  anomalous_data_indices = []\n","  for data_idx in range(TIME_STEPS - 1, len(transformed_data) - TIME_STEPS + 1):\n","      if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n","          anomalous_data_indices.append(data_idx)\n","  anomalies =np.array([-1]*df.shape[0])\n","  anomalies[anomalous_data_indices] =1\n","  df['anomaly']=anomalies\n","  \n","  \n","  #Ingest data to ADX\n","  cluster = \"https://adxc01.westus.kusto.windows.net\"\n","  cluster_ingest_uri = \"https://ingest-adxc01.westus.kusto.windows.net\"\n","  kcsb_ingest = KustoConnectionStringBuilder.with_aad_application_key_authentication(cluster_ingest_uri, client_id, client_secret, authority_id)\n","  queue_client = QueuedIngestClient(kcsb_ingest)\n","  ingestion_props = IngestionProperties(\n","  database=f\"{database_name}\",\n","  table=f\"{table_name}\",\n","  )\n","  queue_client.ingest_from_dataframe(df, ingestion_properties=ingestion_props)\n","  return df\n","    \n","  \n","def process_batch(batchdf, batchid):\n","#   if batchdf.count()< TIME_STEPS:\n","#     return\n","  result_df = batchdf.groupby(\"timestamp\").applyInPandas(detect_anomaly_autoencoder, schema)\n","  \n","  #Write detected anomaly to eventhub for notification\n","  result_df.filter(\"anomaly = 1\").selectExpr(\"to_json(struct(anomaly,location,car_type, count,timestamp)) value\") \\\n","  .write \\\n","  .format(\"kafka\") \\\n","  .option(\"kafka.bootstrap.servers\", \"ehns001.servicebus.windows.net:9093\") \\\n","  .option(\"topic\", \"anomaly_queue\") \\\n","  .option(\"kafka.sasl.mechanism\",\"PLAIN\") \\\n","  .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n","  .option(\"kafka.sasl.jaas.config\", EH_SASL ) \\\n","  .save()\n","  #Write general result to Delta for visualization\n","  result_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Anomany_Result\")\n","\n","\n","\n","  \n","query_df = spark.sql(\"select location, window(starttime, '1 minute').start timestamp, car_type, count(*) count from availability group by window(starttime, '1 minute'), car_type, location\")\n","writer = query_df.writeStream.outputMode(\"complete\").foreachBatch(process_batch)\n","writer.start()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"7b244952-d9e0-44e9-a68c-67dcb8664546","showTitle":false,"title":""}},"outputs":[],"source":["%sql select * from Anomany_Result where anomaly <> -1"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9ab8dc35-d5e4-45d1-b735-8c313a839262","showTitle":false,"title":""}},"source":["### Scoring Isolation Forest Model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"eea9fb1e-e42a-48c7-b0c0-7f7213457b52","showTitle":false,"title":""}},"outputs":[],"source":["import mlflow\n","import mlflow.sklearn\n","import pandas as pd\n","from azure.kusto.data import KustoClient, KustoConnectionStringBuilder\n","from azure.kusto.ingest import (\n","    QueuedIngestClient,\n","    IngestionProperties,\n","    KustoStreamingIngestClient,\n","    ManagedStreamingIngestClient\n","\n",")\n","\n","database_name =\"db01\"\n","table_name = \"anomaly_output\"\n","client_id = dbutils.secrets.get(\"scope01\", \"app01-id\")\n","client_secret = dbutils.secrets.get(\"scope01\", \"app01-sec\")\n","authority_id = \"0fbe7234-45ea-498b-b7e4-1a8b2d3be4d9\"\n","\n","schema = \"anomaly integer, location string, car_type string, count integer,timestamp timestamp\"\n","\n","def detect_anomaly_isolation(iterator):\n","  model = mlflow.sklearn.load_model(\"/dbfs/models/isolation_forest/isolation_forest\")\n","  \n","  cluster = \"https://adxc01.westus.kusto.windows.net\"\n","  cluster_ingest_uri = \"https://ingest-adxc01.westus.kusto.windows.net\"\n","\n","  kcsb_ingest = KustoConnectionStringBuilder.with_aad_application_key_authentication(cluster_ingest_uri, client_id, client_secret, authority_id)\n","  queue_client = QueuedIngestClient(kcsb_ingest)\n","  ingestion_props = IngestionProperties(\n","  database=f\"{database_name}\",\n","  table=f\"{table_name}\",\n","  )\n","  \n","  for df in iterator:\n","    df.starttime = pd.to_datetime(df.starttime)\n","    df['hour'] = df.starttime.dt.hour\n","    df_transformed = df[['location','car_type', 'hour','count']]\n","    result = model.predict(df_transformed)\n","    df_transformed['anomaly'] = result\n","    df_transformed['timestamp'] = df['starttime']\n","    df_transformed = df_transformed[['anomaly','location','car_type', 'count','timestamp']]\n","    queue_client.ingest_from_dataframe(df_transformed, ingestion_properties=ingestion_props)\n","\n","    yield df_transformed\n","    \n","  \n","def process_batch(batchdf, batchid):\n","  result_df = batchdf.mapInPandas(detect_anomaly_isolation, schema)\n","  #Write detected anomaly to eventhub for notification\n","  result_df.filter(\"anomaly = -1\").selectExpr(\"to_json(struct(anomaly,location,car_type, count,timestamp)) value\") \\\n","  .write \\\n","  .format(\"kafka\") \\\n","  .option(\"kafka.bootstrap.servers\", \"ehns001.servicebus.windows.net:9093\") \\\n","  .option(\"topic\", \"anomaly_queue\") \\\n","  .option(\"kafka.sasl.mechanism\",\"PLAIN\") \\\n","  .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n","  .option(\"kafka.sasl.jaas.config\", EH_SASL ) \\\n","  .save()\n","  #Write general result to ADX\n","\n","\n","\n","  \n","query_df = spark.sql(\"select location, window(starttime, '1 minute').start starttime, car_type, count(*) count from availability group by window(starttime, '1 minute'), car_type, location\")\n","writer = query_df.writeStream.outputMode(\"complete\").foreachBatch(process_batch)\n","writer.start()"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":4413512885398573,"dataframes":["_sqldf"]},"pythonIndentUnit":2},"notebookName":"streaming_anomaly_detection","notebookOrigID":4413512885398569,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
